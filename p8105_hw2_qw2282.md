P8105\_hw2\_qw2282
================
Qinyao Wu
9/28/2018

``` r
#Import the data
nyc_transit_data = read.csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, starts_with("route"), entry, vending, entrance_type, starts_with("ada")) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))
```

This data set contains 20 variables, which are line, station\_name, station\_latitude, station\_longitude, route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11, entry, vending, entrance\_type, ada, ada\_notes. These data are stored in a dataframe with 1868 rows and 20 columns. Up until now, I have cleaned up the variable names right after I imported the data from the data file. And I select the important columns and disregard the rest. and I have recode the content under the entry variable by changing the "Yes" and "No" into logical values TRUE and FALSE. These data are not tidy yet because the there still a lot of unnecessary variables, such as the eleven different routes that can be combined into one variable.

``` r
#Find distinct stations. 
distinct_station = distinct(nyc_transit_data, station_name, line, .keep_all = TRUE)
dim(distinct_station)  #465 distinct stations that have different station name, route and lines.
```

    ## [1] 465  20

``` r
ada_com_distinction_station = filter(distinct_station, ada == TRUE)
dim(ada_com_distinction_station) #84 are ada complimented among these different stations. 
```

    ## [1] 84 20

``` r
#Calculate proportion of the stations that do not have vending machine.
proportion = nrow(filter(nyc_transit_data, vending == "NO", entry == TRUE)) / nrow(filter(nyc_transit_data, vending == "NO")) #37.7%
```

There are a total of 465 different stations, some of them are on the same street but serving different routes and lines. 84 of these distinct stations are ADA compliant. 37.704918% of station entrances/ exits without vending that allow entrance.

``` r
#Combine the columns of routes the station serves.
transitdata_tidy = gather(nyc_transit_data, key = route_number, value = route_name, route1:route11)
```

    ## Warning: attributes are not identical across measure variables;
    ## they will be dropped

``` r
#Calculate the station that serves A train. 
station_A_route = 
  distinct(transitdata_tidy, station_name, line, .keep_all = TRUE) %>% 
  filter(route_name == "A") %>% #60 distinct stations serve A
  filter(ada == TRUE) #17 of them have ada.
```

There are a total of 60 distinct stations serve the A train. Of these stations, 17 are ADA complimant.

Problem 2
---------

``` r
#Import the data for Mr. trash wheeel and do the initial cleaning of the names. 
trash_wheel_data = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 1, range = "A2:N338") %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>%  #Removes the lines that do not have a dumster
  mutate(sports_balls = as.integer(round(sports_balls, 0))) #Round sports balls

#Import the data for precpitation 2016.
precipitation_2016 = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 5, range = "A2:B15") %>% 
  janitor::clean_names() %>% 
  filter(!is.na(total)) %>% #Omit the lines without precipitation data. 
  mutate(year = 2016) #Add a variable of year. 
 
#Import the data for precpitation 2017. 
precipitation_2017 = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 4, range = "A2:B15") %>% 
  janitor::clean_names() %>% 
  filter(!is.na(total)) %>% #Omit the lines without precipitation data. 
  mutate(year = 2017) #Add a variable of year. 

#Combine preciptation 2016 and 2017. 
precipitation_tidy = bind_rows(precipitation_2016, precipitation_2017) %>%
  janitor::clean_names() %>% 
  mutate(month = month.name[month])
```

Mr. Trash Wheel data is stored in a dataframe with 285 rows and 14 columns. The dataset contains the following variables: dumpster, month, year, date, weight\_tons, volume\_cubic\_yards, plastic\_bottles, polystyrene, cigarette\_butts, glass\_bottles, grocery\_bags, chip\_bags, sports\_balls, homes\_powered. The rows indicates there are a total of 285 observations from 2014. Some key variables are the dumpsters, volume-cubic\_yards, weight\_tons and year. This variables can be used to depict the dataset. And some other cariables, such as the glass bottles and plastic bottles are not key variables. This original data set had been cleaned up by changing the names to a better format and removed lines without dumsters.

The precipitations for 2016 and 2017 are stored in two different dataframe.The dataframe of 2016 contains 13 observations and 3 columns. The dataframe of 2016 contains 13 observations and 3 columns. They both containing the following variables: month, total, year. I omitted rows that do not have precipitation data. Two dataframes for precipitation are combined and a new precipitation\_tidy dataframe is created. The total precipitation in 2017 is 32.93. The median number of sports balls in the dumster in 2016 is 26.

Problem 3
---------

``` r
#Import the data. 
data(brfss_smart2010)

#Clean the variable names. 
brfss_smart2010 = janitor::clean_names(brfss_smart2010) %>% 
  filter(topic == "Overall Health") %>% #Focus on the overall health topic. 
  select(-c(topic, class, question, sample_size, confidence_limit_low:geo_location)) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>%  #Clean the names again 
  mutate(proportion = excellent + very_good) #Calculate the proportions and add it as a variable. 


#Calculaye unique locations
distinct_location = distinct(brfss_smart2010, locationdesc, .keep_all = TRUE) #total of 404 different locations

distinct_state = distinct(brfss_smart2010, locationabbr, .keep_all = TRUE) #51 states. 

tail(names(sort(table(brfss_smart2010$locationabbr))), 1) #NJ is the most frequently appeared states.
```

    ## [1] "NJ"

``` r
ggplot(filter(brfss_smart2010, year == 2002), aes(x = excellent)) + geom_histogram()
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 2 rows containing non-finite values (stat_bin).

![](p8105_hw2_qw2282_files/figure-markdown_github/problem%203-1.png)

``` r
ggplot(filter(brfss_smart2010, locationdesc == "NY - New York County" | locationdesc == "NY - Queens County"), aes(x = year, y = excellent)) + geom_point(aes(color = locationdesc))
```

![](p8105_hw2_qw2282_files/figure-markdown_github/problem%203-2.png)

There are a total of 404 locations are included in the dataset. And there are 51 states. Since there are a total of 51 states, including the district of columbia, in United States, all the states are represented in this dataset. The most frequently appeared state is NJ. In 2002, the median of the "Excellent" response value are 23.6.

The two plots, first one is a histogram and second one is a scatter plot. From the histogram, we can see that there are some extreme values for excellent percentage in 2002. And most frequent excellent value is around 20 to 30 percent. From the scatterplot, we can obviously see the difference btween the New York County and Queen County. The New York County has generally higher excellent percentage throughout the years compared with the Queens County.
